{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## setting random seed for easy comparison\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_env = gym.make('FrozenLake-v0')\n",
    "big_env = gym.make('FrozenLake20x20-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_value(grid, rows, cols, val):\n",
    "    grid[(rows),(cols)] = val\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_world_grid(env):\n",
    "    grid_array = np.zeros(env.desc.shape)\n",
    "    rows, cols = np.where(env.desc == b'H')\n",
    "    grid_array = fill_value(grid_array, rows, cols, 0)\n",
    "    rows, cols = np.where(env.desc == b'S')\n",
    "    grid_array = fill_value(grid_array, rows, cols, 1)\n",
    "    rows, cols = np.where(env.desc == b'F')\n",
    "    grid_array = fill_value(grid_array, rows, cols, 2)\n",
    "    rows, cols = np.where(env.desc == b'G')\n",
    "    grid_array = fill_value(grid_array, rows, cols, 3)\n",
    "    return grid_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_colors = ['#99976C','#FFD967','#A7CDFF','#67C759']\n",
    "cmap=colors.ListedColormap(grid_colors)\n",
    "color_meaning = ['Hole','Start','Frozen','Goal']\n",
    "patches = [mpatches.Patch(color=grid_colors[i], label=color_meaning[i] ) for i in range(4) ]\n",
    "#     plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_env.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1 = plt.figure(figsize=(5,5))\n",
    "g = make_world_grid(small_env)\n",
    "im = plt.imshow(g, cmap=cmap)\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(-.5, 3.5, 1))\n",
    "ax.set_yticks(np.arange(-0.5, 3.5, 1))\n",
    "ax.grid(color='black', linestyle='-', linewidth=2)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.title('4x4 Frozen Lake')\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1 = plt.figure(figsize=(5,5))\n",
    "g = make_world_grid(big_env)\n",
    "im1 = plt.imshow(g, cmap=cmap)\n",
    "ax1 = plt.gca()\n",
    "ax1.set_xticks(np.arange(-.5, 20.5, 1))\n",
    "ax1.set_yticks(np.arange(-0.5, 20.5, 1))\n",
    "ax1.grid(color='black', linestyle='-', linewidth=2)\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_xticklabels([])\n",
    "plt.title('20x20 Frozen Lake')\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards:\n",
    "1. Moving = -1\n",
    "2. falling in hole = -10\n",
    "3. goal = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_policy(env,v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    print(env)\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 10000\n",
    "    eps = 1e-20\n",
    "    \n",
    "    values = []\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [gamma *sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "              \n",
    "#         test_value = (np.mean(np.fabs((prev-v)**2)))**0.5\n",
    "        values.append(np.sum(np.fabs(prev_v - v)))\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    changes = []\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(env,old_policy_v, gamma)\n",
    "        changes.append(np.sum(policy != new_policy))\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy, changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode_q(env,Q,learning_rate,discount,epsilon,render=False):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    t_reward = 0\n",
    "    max_steps = 1000\n",
    "    for i in range(max_steps):     \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        curr_state = observation\n",
    "\n",
    "\n",
    "\n",
    "        # exploration\n",
    "        if(np.random.rand(1) < epsilon):\n",
    "            action = np.argmax(Q[curr_state,:]  + np.random.randn(1, env.action_space.n)*(1./(i+1)))\n",
    "        else:\n",
    "            #exploitation\n",
    "            action = np.argmax(Q[curr_state,:])\n",
    "\n",
    "            \n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        t_reward += reward\n",
    "\n",
    "        #Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "        Q[curr_state,action] += learning_rate * (reward+ discount*np.max(Q[observation,:])-Q[curr_state,action])\n",
    "    \n",
    "    return Q, t_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_policy_from_q_table(q_table):\n",
    "    policy = np.zeros(len(q_table))\n",
    "    for index, row in enumerate(q_table):\n",
    "        policy[index] = np.argmax(row)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(env, discount, epsilon):\n",
    "    num_episodes = 10000\n",
    "    learning_rate = 1.0  \n",
    "    sums = []\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for i in range(num_episodes):\n",
    "        prev_Q = np.copy(Q)\n",
    "        Q,reward = run_episode_q(env,Q,learning_rate,discount,epsilon)\n",
    "        learning_rate = learning_rate - 0.1 * (1/(1+i))\n",
    "\n",
    "#         sums.append(np.sum(Q))\n",
    "        sums.append(np.sum(np.fabs(prev_Q - Q)))\n",
    "    \n",
    "    \n",
    "    return Q, sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration vs value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_gammas(gammas, env, method, title, ylabel):\n",
    "    policies = []\n",
    "    for gamma in gammas:\n",
    "        if method == 'value':\n",
    "            optimal_v, values = value_iteration(env, gamma=gamma)\n",
    "            policy_v = extract_policy(env,optimal_v, gamma=gamma)\n",
    "            policies.append(policy_v)\n",
    "            \n",
    "        if method == 'policy':\n",
    "            policy, values = policy_iteration(env, gamma=gamma)\n",
    "            policies.append(policy)\n",
    "            \n",
    "        plt.plot(values,label='gamma={}'.format(gamma))\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel(ylabel)\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gammas = [1.0, 0.9, 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lake_value_policies = compare_gammas(gammas,\n",
    "                                           small_env,\n",
    "                                           'value',\n",
    "                                           'Various Discount Factors for Value Iteration 4x4 Lake',\n",
    "                                           'Sum of Differeces Between Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lake_policy_policies = compare_gammas(gammas,\n",
    "                                           small_env,\n",
    "                                           'policy',\n",
    "                                           'Various Discount Factors for Policy Iteration 4x4 Lake',\n",
    "                                           'Number of Actions Updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_lake_value_policies = compare_gammas(gammas,\n",
    "                                           big_env,\n",
    "                                           'value',\n",
    "                                           'Various Discount Factors for Value Iteration 20x20 Lake',\n",
    "                                           'Sum of Differeces Between Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_lake_policy_policies = compare_gammas(gammas,\n",
    "                                           big_env,\n",
    "                                           'policy',\n",
    "                                           'Various Discount Factors for Policy Iteration 20x20 Lake',\n",
    "                                           'Number of Actions Updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examin policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_colors = ['black', '#9EE6FF','#304799','#FFEADE','#CC7F6A','#67FF6C']\n",
    "cmap=colors.ListedColormap(grid_colors)\n",
    "color_meaning = ['Hole','Left','Down','Right','Up','Goal']\n",
    "def graph_policy(env, policy, lake_size, title):\n",
    "    figure = plt.figure(figsize=(5,5))\n",
    "    policy = np.reshape(policy, env.desc.shape)\n",
    "    ##find holes\n",
    "    rows, cols = np.where(env.desc == b'H')\n",
    "    policy = fill_value(policy, rows, cols, -1)\n",
    "    rows, cols = np.where(env.desc == b'G')\n",
    "    policy = fill_value(policy, rows, cols, 4)\n",
    "    plt.imshow(policy, cmap=cmap)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    patches = [mpatches.Patch(color=grid_colors[i], label=color_meaning[i] ) for i in range(6) ]\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "    \n",
    "    if lake_size == 'small':\n",
    "        ax.set_xticks(np.arange(-0.5, 3.5, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, 3.5, 1))\n",
    "    elif lake_size == 'big':\n",
    "        ax.set_xticks(np.arange(-0.5, 20.5, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, 20.5, 1))\n",
    "        \n",
    "    ax.grid(color='black', linestyle='-', linewidth=2)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.title(title)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['value iteration - gamma = 1.0', 'value iteration - gamma = 0.9','value iteration gamma = 0.75']\n",
    "for index, pol in enumerate(small_lake_value_policies):\n",
    "    graph_policy(small_env, pol, 'small',titles[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['polcy iteration - gamma = 1.0', 'policy iteration - gamma = 0.9','policy iteration gamma = 0.75']\n",
    "for index, pol in enumerate(small_lake_policy_policies):\n",
    "    graph_policy(small_env, pol, 'small',titles[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['value iteration - gamma = 1.0', 'value iteration - gamma = 0.9','value iteration gamma = 0.75']\n",
    "for index, pol in enumerate(big_lake_value_policies):\n",
    "    graph_policy(big_env, pol,'big',titles[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['policy iteration - gamma = 1.0', 'policy iteration - gamma = 0.9','policy iteration gamma = 0.75']\n",
    "for index, pol in enumerate(big_lake_policy_policies):\n",
    "    graph_policy(big_env, pol,'big',titles[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_epsilons(epsilons, env,title):\n",
    "    policies = []\n",
    "    discount = 0.9\n",
    "    for epsilon in epsilons:\n",
    "        q, values = train(env, discount,epsilon )\n",
    "        sampled_values = values[0::50]\n",
    "        policy = extract_policy_from_q_table(q)\n",
    "        policies.append(policy)\n",
    "         \n",
    "        x = np.linspace(0,10000,num=200,endpoint=False)    \n",
    "        plt.scatter(x,sampled_values,label='epsilon={}'.format(epsilon))\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('sum of difference between iterations')\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilons = [0.1, 0.5, 0.9]\n",
    "q_small_polcies = compare_epsilons(epsilons, small_env, 'Q-learning various exploration probabilities - 4x4 Lake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_big_polcies = compare_epsilons(epsilons, big_env, 'Q-learning various exploration probabilities-  20x20 Lake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['q-learning - epsilon = 0.1', 'q-learning - epsilon = 0.5','q-learning - epsilon = 0.9']\n",
    "for index, pol in enumerate(q_small_polcies):\n",
    "    graph_policy(small_env, pol,'small',titles[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['q-learning - epsilon = 0.1', 'q-learning - epsilon = 0.5','q-learning - epsilon = 0.9']\n",
    "for index, pol in enumerate(q_big_polcies):\n",
    "    graph_policy(big_env, pol,'big',titles[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
